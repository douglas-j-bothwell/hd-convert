type: article
article_id: 3j87za7lho
user_id: mfr0nxh4be
category_id: gul20j54zg
author:
  name: Michael Cretzman
  profile_image: https://www.gravatar.com/avatar/2e8616837f4ee92be5d19ffe9b9ccba9?d=mm&s=150
title: OpenShift Connected On-Prem Setup
slug: open-shift-connected-on-prem-setup
description: Run the Harness Continuous Delivery-as-a-Service platform on-premise,
  in a OpenShift cluster.
short_version: Run the Harness Continuous Delivery-as-a-Service platform on-premise,
  in a OpenShift cluster.
tags:
- on-premise
- OpenShift
- kubeconfig
- YAML
- kubectl
- Ambassador
show_toc: true
is_private: false
is_published: false
is_featured: false
stale_status:
  is_stale: false
  reason: ""
  source: API
  triggered_at: 2022-04-25T20:42:46.391864Z
  expires_at: null
permission_groups: []
multilingual:
- language_code: en
  title: OpenShift Connected On-Prem Setup
  description: Run the Harness Continuous Delivery-as-a-Service platform on-premise,
    in a OpenShift cluster.
  short_version: Run the Harness Continuous Delivery-as-a-Service platform on-premise,
    in a OpenShift cluster.
  body: |-
    <p>In addition to the Harness SaaS offering, you can run Harness on-premise, in an
      <a href="https://docs.openshift.com/?extIdCarryOver=true&amp;sc_cid=701f2000001Css5AAC">OpenShift</a> cluster.</p>
    <p>In this topic:</p>
    <ul>
      <li>
        <a href="#architecture">Architecture</a>
      </li>
      <li>
        <a href="#prerequisites">Prerequisites</a>
      </li>
      <li>
        <a href="#system_requirements">System Requirements</a>
      </li>
      <li>
        <a href="#connectivity_and_access_requirements">Connectivity and Access Requirements</a>
      </li>
      <li>
        <a href="#open_shift_cluster_setup">OpenShift Cluster Setup</a>
        <ul>
          <li>
            <a href="#yaml_for_open_shift">YAML for OpenShift</a>
          </li>
          <li>
            <a href="#security_context_constraints_sc_cs">Security Context Constraints (SCCs) </a>
          </li>
          <li>
            <a href="#kube_config_file">Kube Config File</a>
          </li>
          <li>
            <a href="#docker_repository_setup">Docker Repository Setup</a>
          </li>
          <li>
            <a href="#ambassador_virtual_machine_setup">Ambassador Virtual Machine Setup</a>
            <ul>
              <li>
                <a href="#connectivity">Connectivity to the Harness Cloud</a>
              </li>
              <li>
                <a href="#proxy_size">Proxy File-Size Limit</a>
              </li>
              <li>
                <a href="#openshift_repository">Docker Integrated OpenShift Container Registry (OCR)</a>
              </li>
              <li>
                <a href="#other_docker_repository">Other Docker Repository</a>
              </li>
              <li>
                <a href="#kubectl_command">kubectl Command</a>
              </li>
              <li>
                <a href="#secret">Secret</a>
              </li>
            </ul>
          </li>
          <li>
            <a href="#load_balancer_setup"> Load Balancer Setup</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="#installation">Installation</a>
      </li>
      <li>
        <a href="#verification">Post-Installation Steps</a>
      </li>
      <li>
        <a href="#next_steps">Next Steps</a>
      </li>
    </ul>
    <p></p>
    <div class="note-callout">Harness on-prem installations do not currently support the Harness
      <a href="/article/6n7fon8rit-using-the-helm-delegate">Helm Delegate</a>.</div>
    <h3>Architecture</h3>
    <p>Here is the high-level architecture of Harness Connected On-Premise with OpenShift:</p>
    <p></p>
    <figure>
      <img src="https://files.helpdocs.io/kw8ldg1itf/articles/3j87za7lho/1587401682742/image.png"/>
    </figure>
    <p></p>
    <p>The main components are:</p>
    <ul>
      <li>OpenShift cluster hosting the microservices used by Harness, such as the Harness Manager, MongoDB, Verification, and Machine Learning Engine.</li>
      <li>Docker repo used to host the Harness microservice images pulled.</li>
      <li>Harness Ambassador hosted on a Virtual Machine (VM) that makes a secure outbound connection to the Harness Cloud to download binaries.</li>
    </ul>
    <h3>Prerequisites</h3>
    <p>This section lists the infrastructure requirements that must be in place before following the steps in
      <a href="https://docs.harness.io/article/3j87za7lho-open-shift-connected-on-prem-setup#installation">Installation</a>.</p>
    <h3>System Requirements</h3>
    <ul>
      <li>OpenShift cluster (v3.11) with a dedicated project: <strong>harness</strong></li>
      <li>30 cores, 52 GB RAM, 910 GB disk storage</li>
    </ul>
    <table>
      <tbody>
        <tr>
          <td>
            <p><strong>Microservice</strong></p>
          </td>
          <td>
            <p><strong>Pods</strong></p>
          </td>
          <td>
            <p><strong>CPU</strong></p>
          </td>
          <td>
            <p><strong>Memory (GB)</strong></p>
          </td>
        </tr>
        <tr>
          <td>
            <p>Manager</p>
          </td>
          <td>
            <p>2</p>
          </td>
          <td>
            <p>4</p>
          </td>
          <td>
            <p>10</p>
          </td>
        </tr>
        <tr>
          <td>
            <p>Verification</p>
          </td>
          <td>
            <p>2</p>
          </td>
          <td>
            <p>2</p>
          </td>
          <td>
            <p>6</p>
          </td>
        </tr>
        <tr>
          <td>
            <p>Machine Learning Engine</p>
          </td>
          <td>
            <p>1</p>
          </td>
          <td>
            <p>8</p>
          </td>
          <td>
            <p>2</p>
          </td>
        </tr>
        <tr>
          <td>
            <p>UI</p>
          </td>
          <td>
            <p>2</p>
          </td>
          <td>
            <p>0.5</p>
          </td>
          <td>
            <p>0.5</p>
          </td>
        </tr>
        <tr>
          <td>
            <p>MongoDB</p>
          </td>
          <td>
            <p>3</p>
          </td>
          <td>
            <p>12</p>
          </td>
          <td>
            <p>24</p>
          </td>
        </tr>
        <tr>
          <td>
            <p>Proxy</p>
          </td>
          <td>
            <p>1</p>
          </td>
          <td>
            <p>0.5</p>
          </td>
          <td>
            <p>0.5</p>
          </td>
        </tr>
        <tr>
          <td>
            <p>Ingress</p>
          </td>
          <td>
            <p>2</p>
          </td>
          <td>
            <p>0.5</p>
          </td>
          <td>
            <p>0.5</p>
          </td>
        </tr>
        <tr>
          <td>
            <p>TimescaleDB</p>
          </td>
          <td>
            <p>3</p>
          </td>
          <td>
            <p>6</p>
          </td>
          <td>
            <p>24</p>
          </td>
        </tr>
        <tr>
          <td>
            <p>Total</p>
          </td>
          <td>
            <p>16</p>
          </td>
          <td>
            <p>33.5</p>
          </td>
          <td>
            <p>67.5</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p> </p>
    <ul>
      <li>Storage volume (200 GB) for each Mongo pod. (600 GB total).</li>
      <li>Storage volume (10 GB) for the Proxy pod. </li>
      <li>Storage volume (100 GB) for each TimeScaleDB pod (300 GB total).</li>
      <li>Docker Repository (for Harness microservice images). This repository could be either:
        <ul>
          <li>Integrated OpenShift Container Registry (OCR)</li>
          <li>Internal Hosted Repository</li>
        </ul>
      </li>
      <li>Harness Ambassador: 1 core, 6GB RAM, 20 GB Disk.</li>
      <li>Root access on localhost is required only for Docker. If root access is not possible, please follow the instructions on how to run Docker as a non-root user:
        <a href="https://docs.docker.com/install/linux/linux-postinstall/">https://docs.docker.com/install/linux/linux-postinstall/</a>.</li>
    </ul>
    <h3>Connectivity and Access Requirements</h3>
    <ul>
      <li>Load balancer to OpenShift cluster.</li>
      <li>OpenShift cluster to Docker repository to pull images.</li>
      <li>Ambassador to OpenShift cluster to run kubectl and oc commands.</li>
      <li>Ambassador to Docker repository to push Harness images.</li>
      <li>Ambassador to load balancer.</li>
      <li>Ambassador to app.harness.io (port: 443).</li>
    </ul>
    <h4>Trusted Certificate Requirement for Harness On-Prem</h4>
    <p>All connections to the Harness Manager can be secure or unencrypted according to the URL scheme you use when you configure the Load Balancer URL during installation (<code>https://</code> or <code>http://</code>):</p>
    <p>For secure connections from any integration into the Harness Manager (Github Webhooks, etc), including the <strong>Harness Delegate</strong>, you must use a
      <u>publicly trusted certificate</u>.</p>
    <p>Harness does not support self-signed certificates for connections to the Harness Manager.</p>
    <p>For connections from the Harness Manager outbound to an integration, you can use a self-signed certificate. In this case, you must import the self-signed certificate into Harness Delegate&#39;s JRE keystore manually or using a Harness
      <a href="https://docs.harness.io/article/yd4bs0pltf-run-scripts-on-the-delegate-using-profiles">Delegate Profile</a>.</p>
    <p>See
      <a href="https://docs.harness.io/article/8bj3v5jqzk-add-self-signed-certificates-for-delegate-connections">Add Self-Signed Certificates for Delegate Connections</a>.</p>
    <h3>OpenShift Cluster Setup</h3>
    <p>The following resources must be created in the OpenShift cluster:</p>
    <ul>
      <li><strong>Project</strong> - Must be named <code>harness</code>. You can use the following command:</li>
    </ul><pre> oc new-project harness</pre>
    <ul>
      <li><strong>Service account</strong> - The Kubernetes Service account must have all permissions within the harness project.</li>
      <li><strong>Role</strong> - A role that provides access on the required API groups (apiGroups).</li>
      <li><strong>RoleBinding</strong> - The
        <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#default-roles-and-role-bindings">RoleBinding</a> between the service account and the role that provides access on the required API groups.</li>
      <li><strong>StorageClass</strong> - The Harness installer will leverage the
        <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">StorageClass</a> configured in the cluster.</li>
    </ul>
    <h4>YAML for OpenShift</h4>
    <p>The following YAML code can be executed by the cluster admin to create the necessary resources using kubectl apply or oc apply commands.</p><pre>oc apply -n harness -f &lt;FILENAME&gt;.yaml</pre>
    <p> Here are the contents of <strong>&lt;FILENAME&gt;.yaml</strong>:</p><pre>apiVersion: v1<br/>kind: ServiceAccount<br/>metadata:<br/>  name: <strong>harness-namespace-admin</strong><br/>  namespace: harness<br/>---<br/>kind: Role<br/>apiVersion: rbac.authorization.k8s.io/v1<br/>metadata:<br/>  name: <strong>harness-namespace-admin</strong>-full-access<br/>  namespace: harness<br/>rules:<br/>- apiGroups: [&#34;&#34;, &#34;extensions&#34;, &#34;apps&#34;, &#34;autoscaling&#34;, &#34;rbac.authorization.k8s.io&#34;, &#34;roles.rbac.authorization.k8s.io&#34;, &#34;route.openshift.io&#34;]<br/>  resources: [&#34;*&#34;]<br/>  verbs: [&#34;*&#34;]<br/>- apiGroups: [&#34;batch&#34;]<br/>  resources:<br/>  - jobs<br/>  - cronjobs<br/>  verbs: [&#34;*&#34;]<br/>---<br/>kind: RoleBinding<br/>apiVersion: rbac.authorization.k8s.io/v1<br/>metadata:<br/>  name: <strong>harness-namespace-admin</strong>-view<br/>  namespace: harness<br/>subjects:<br/>- kind: ServiceAccount<br/>  name: <strong>harness-namespace-admin</strong><br/>  namespace: harness<br/>roleRef:<br/>  apiGroup: rbac.authorization.k8s.io<br/>  kind: Role<br/>  name: <strong>harness-namespace-admin</strong>-full-access<br/>---<br/>apiVersion: rbac.authorization.k8s.io/v1<br/>kind: ClusterRole<br/>metadata:<br/>  name: harness-clusterrole<br/>  labels:<br/>    app.kubernetes.io/name: harness<br/>    app.kubernetes.io/part-of: harness<br/>rules:<br/>  - apiGroups:<br/>      - &#34;&#34;<br/>    resources:<br/>      - configmaps<br/>      - endpoints<br/>      - nodes<br/>      - pods<br/>      - secrets<br/>    verbs:<br/>      - list<br/>      - watch<br/>  - apiGroups:<br/>      - &#34;&#34;<br/>    resources:<br/>      - nodes<br/>    verbs:<br/>      - get<br/>  - apiGroups:<br/>      - &#34;&#34;<br/>    resources:<br/>      - services<br/>    verbs:<br/>      - get<br/>      - list<br/>      - watch<br/>  - apiGroups:<br/>      - &#34;extensions&#34;<br/>    resources:<br/>      - ingresses<br/>    verbs:<br/>      - get<br/>      - list<br/>      - watch<br/>  - apiGroups:<br/>      - &#34;&#34;<br/>    resources:<br/>      - events<br/>    verbs:<br/>      - create<br/>      - patch<br/>  - apiGroups:<br/>      - &#34;extensions&#34;<br/>    resources:<br/>      - ingresses/status<br/>    verbs:<br/>      - update<br/>---<br/>apiVersion: v1<br/>kind: ServiceAccount<br/>metadata:<br/>  name: harness-serviceaccount<br/>  namespace: harness<br/>  labels:<br/>    app.kubernetes.io/name: harness<br/>    app.kubernetes.io/part-of: harness<br/>---<br/>apiVersion: rbac.authorization.k8s.io/v1<br/>kind: ClusterRoleBinding<br/>metadata:<br/>  name: harness-clusterrole-hsa-binding<br/>  labels:<br/>    app.kubernetes.io/name: harness<br/>    app.kubernetes.io/part-of: harness<br/>roleRef:<br/>  apiGroup: rbac.authorization.k8s.io<br/>  kind: ClusterRole<br/>  name: harness-clusterrole<br/>subjects:<br/>  - kind: ServiceAccount<br/>    name: harness-serviceaccount<br/>    namespace: harness</pre>
    <h4>Security Context Constraints (SCCs) </h4>
    <p>Allow service accounts (created above) in “<strong>harness</strong>” project to run Docker images as any user:</p><pre>oc adm policy add-scc-to-user anyuid -z harness-serviceaccount -n harness</pre><pre>oc adm policy add-scc-to-user anyuid -z harness-default -n harness</pre>
    <h4>Kube Config File</h4>
    <p>Now that the project and service account are created, we need a
      <a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubeconfig</a> file for the Ambassador to use that has the permissions of the <code>harness-namespace-admin</code> service account. You can use your own utility to create the kubeconfig file, or use the script below.</p>
    <p>In the following script, <strong>generate_kubeconfig.sh</strong>, you need to add the secret name of the <code>harness-namespace-admin</code> service account in the <strong>SECRET_NAME</strong> variable. To get the secret name associated with the harness-namespace-admin
      service account, run this command:</p><pre>oc get sa <strong>harness-namespace-admin</strong> -n harness -o json | jq -r &#39;.secrets&#39;</pre>
    <p>From the output of this command, pick the secret other than dockercfg secret.</p>
    <ol>
      <li>Copy and paste the following into a file called generate_kubeconfig.sh. </li>
    </ol><pre>set -e<br/>set -o pipefail<br/><br/>SERVICE_ACCOUNT_NAME=<strong>harness-namespace-admin</strong><br/>NAMESPACE=harness<br/>SECRET_NAME=&lt;Put here the name of the secret associated with SERVICE_ACCOUNT_NAME&gt;<br/>KUBECFG_FILE_NAME=&#34;./kube/k8s-${SERVICE_ACCOUNT_NAME}-${NAMESPACE}-conf&#34;<br/>TARGET_FOLDER=&#34;./kube/&#34;<br/><br/>create_target_folder() {<br/>    echo -n &#34;Creating target directory to hold files in ${TARGET_FOLDER}...&#34;<br/>    mkdir -p &#34;${TARGET_FOLDER}&#34;<br/>    printf &#34;done&#34;<br/>}<br/><br/>extract_ca_crt_from_secret() {<br/>    echo -e -n &#34;\\nExtracting ca.crt from secret...&#34;<br/>    kubectl get secret &#34;${SECRET_NAME}&#34; --namespace &#34;${NAMESPACE}&#34; -o json | jq \<br/>    -r &#39;.data[&#34;ca.crt&#34;]&#39; | base64 -d &gt; &#34;${TARGET_FOLDER}/ca.crt&#34;<br/>    printf &#34;done&#34;<br/>}<br/><br/>get_user_token_from_secret() {<br/>    echo -e -n &#34;\\nGetting user token from secret...&#34;<br/>    USER_TOKEN=$(kubectl get secret &#34;${SECRET_NAME}&#34; \<br/>    --namespace &#34;${NAMESPACE}&#34; -o json | jq -r &#39;.data[&#34;token&#34;]&#39; | base64 -d)<br/>    printf &#34;done&#34;<br/>}<br/><br/>set_kube_config_values() {<br/>    context=$(kubectl config current-context)<br/>    echo -e &#34;\\nSetting current context to: $context&#34;<br/><br/>    CLUSTER_NAME=$(kubectl config get-contexts &#34;$context&#34; | awk &#39;{print $3}&#39; | tail -n 1)<br/>    echo &#34;Cluster name: ${CLUSTER_NAME}&#34;<br/><br/>    ENDPOINT=$(kubectl config view \<br/>    -o jsonpath=&#34;{.clusters[?(@.name == \&#34;${CLUSTER_NAME}\&#34;)].cluster.server}&#34;)<br/>    echo &#34;Endpoint: ${ENDPOINT}&#34;<br/><br/>    # Set up the config<br/>    echo -e &#34;\\nPreparing k8s-${SERVICE_ACCOUNT_NAME}-${NAMESPACE}-conf&#34;<br/>    echo -n &#34;Setting a cluster entry in kubeconfig...&#34;<br/>    kubectl config set-cluster &#34;${CLUSTER_NAME}&#34; \<br/>    --kubeconfig=&#34;${KUBECFG_FILE_NAME}&#34; \<br/>    --server=&#34;${ENDPOINT}&#34; \<br/>    --certificate-authority=&#34;${TARGET_FOLDER}/ca.crt&#34; \<br/>    --embed-certs=true<br/><br/>    echo -n &#34;Setting token credentials entry in kubeconfig...&#34;<br/>    kubectl config set-credentials \<br/>    &#34;${SERVICE_ACCOUNT_NAME}-${NAMESPACE}-${CLUSTER_NAME}&#34; \<br/>    --kubeconfig=&#34;${KUBECFG_FILE_NAME}&#34; \<br/>    --token=&#34;${USER_TOKEN}&#34;<br/><br/>    echo -n &#34;Setting a context entry in kubeconfig...&#34;<br/>    kubectl config set-context \<br/>    &#34;${SERVICE_ACCOUNT_NAME}-${NAMESPACE}-${CLUSTER_NAME}&#34; \<br/>    --kubeconfig=&#34;${KUBECFG_FILE_NAME}&#34; \<br/>    --cluster=&#34;${CLUSTER_NAME}&#34; \<br/>    --user=&#34;${SERVICE_ACCOUNT_NAME}-${NAMESPACE}-${CLUSTER_NAME}&#34; \<br/>    --namespace=&#34;${NAMESPACE}&#34;<br/><br/>    echo -n &#34;Setting the current-context in the kubeconfig file...&#34;<br/>    kubectl config use-context &#34;${SERVICE_ACCOUNT_NAME}-${NAMESPACE}-${CLUSTER_NAME}&#34; \<br/>    --kubeconfig=&#34;${KUBECFG_FILE_NAME}&#34;<br/>}<br/><br/>create_target_folder<br/>extract_ca_crt_from_secret<br/>get_user_token_from_secret<br/>set_kube_config_values<br/><br/>echo -e &#34;\\nAll done! Testing with:&#34;<br/>echo &#34;KUBECONFIG=${KUBECFG_FILE_NAME} kubectl get pods -n ${NAMESPACE}&#34;<br/>KUBECONFIG=${KUBECFG_FILE_NAME} kubectl get pods -n ${NAMESPACE}</pre>
    <p>When you execute this script it will create a folder called <strong>kube</strong> that contains a file named <strong>k8s-harness-namespace-admin-harness-conf</strong>. This file needs to be copied on the Ambassador VM as <code>$HOME/.kube/config</code>  (see below).</p>
    <h4>Docker Repository Setup</h4>
    <p>A Docker repository must be created on a repository server to host Docker images of Harness microservices. Integrated OpenShift Container Registry (OCR) can also be used to host Docker images of Harness microservices.</p>
    <p>The internal URL of the Docker repository must be sent to Harness as instructed in the <strong>Installation</strong> section.</p>
    <h4>Ambassador Virtual Machine Setup</h4>
    <p>The Harness Ambassador is the only required component that needs <strong>outbound only</strong> connectivity to <strong>app.harness.io:443</strong>. The Harness Cloud uses Ambassador to manage the installation of Harness On-Prem. Ambassador has the following
      connectivity and access requirements.</p>
    <div class="hd--md" data-hd-markdown="&lt;span id=&#34;connectivity&#34;&gt;&lt;/span&gt;">
      <p><span id="connectivity"></span></p>
    </div>
    <h5>Connectivity to the Harness Cloud</h5>
    <p>Connectivity to the Harness Cloud via hostname <strong>app.harness.io</strong> and <strong>port 443</strong>. The following command will check connectivity to the Harness Cloud:</p><pre>nc -vz app.harness.io 443</pre>
    <p></p>
    <div class="hd--md" data-hd-markdown="&lt;span id=&#34;proxy_size&#34;&gt;&lt;/span&gt;">
      <p><span id="proxy_size"></span></p>
    </div>
    <h5>Proxy File-Size Limit</h5>
    <p>If you have a proxy set up in front of the Ambassador to reach <strong>app.harness.io</strong>, ensure that your proxy&#39;s configuration allows downloads of files as large as 2 GB. This is required to pull the artifacts that the Ambassador will download
      to install and upgrade Harness microservices. On
      <a href="https://httpd.apache.org/docs/2.2/mod/core.html#limitrequestbody">Apache</a>, set this limit using the<code>LimitRequestBody</code>directive. On
      <a href="http://nginx.org/en/docs/http/ngx_http_core_module.html#client_max_body_size">nginx</a>, use the  <code>client_max_body_size</code>directive.</p>
    <p></p>
    <div class="hd--md" data-hd-markdown="&lt;span id=&#34;openshift_repository&#34;&gt;&lt;/span&gt;">
      <p><span id="openshift_repository"></span></p>
    </div>
    <h5>Docker Integrated OpenShift Container Registry (OCR)</h5>
    <p>Run the Docker
      <a href="https://docs.docker.com/engine/reference/commandline/login/">login command</a> with credentials that can access the repository created for the Harness microservices. This command allows the Ambassador to push Docker images to the local repository during installation.</p>
    <p></p>
    <h6>Login</h6>
    <p></p><pre>docker login -p &lt;token&gt; -u unused &lt;ocr_url&gt;</pre>
    <p></p>
    <p></p>
    <h6>Permissions</h6>
    <p></p>
    <p>The token used in above login command should meet the following requirements:</p>
    <ol>
      <li>The token should be for a regular user or a service account.</li>
      <li>The user with which you log into OCR should have following roles:
        <ol>
          <li>registry-viewer</li>
          <li>registry-editor</li>
          <li>system:image-builder</li>
        </ol>
      </li>
    </ol>
    <p>You can use the following command to assign the above roles:</p><pre> oc policy add-role-to-user -n harness &lt;role_name&gt; &lt;user&gt; </pre>
    <p></p>
    <div class="hd--md" data-hd-markdown="&lt;span id=&#34;other_docker_repository&#34;&gt;&lt;/span&gt;">
      <p><span id="other_docker_repository"></span></p>
    </div>
    <h5>Other Docker Repository</h5>
    <p>Use these instructions to set up login on the Ambassador for other Docker repositories.</p>
    <h6>Login</h6>
    <p></p><pre> docker login repository_url</pre>
    <p></p>
    <h6>Permissions</h6>
    <p>The Docker user configured on the Ambassador should have permissions to read, write, and create new repositories on the Docker registry. (If the user lacks the permission to create new repositories, contact
      <a href="mailto:support@harness.io" target="_blank">Harness Support</a> to pre-create the repositories for the installation.)</p>
    <h5>kubectl Command</h5>
    <p>The <strong>kubectl</strong> command interface should be preinstalled and preconfigured to connect to the <code>harness</code> project in the given OpenShift cluster. </p>
    <p>By default, the kubectl command uses the config hosted at <code>$HOME/.kube/config</code> location. Ensure this file is created according to the instructions in the
      <a href="https://docs.harness.io/article/go5nk41qvd-kubernetes-connected-on-prem-setup#kube_config_file">Kube Config File</a> section. The following commands can be run on the VM to ensure connectivity and access to the given namespace of the cluster.</p><pre>kubectl get deploy -n harness<br/><br/>kubectl get pods -n harness<br/><br/>kubectl get hpa -n harness<br/><br/>kubectl get configmap -n harness<br/><br/>kubectl get secret -n harness</pre>
    <h5>Secret</h5>
    <p>Create a secret in the <code>harness</code> project to access the internal Docker repository. This secret can be referenced by the Harness microservices installed later on in
      <a href="https://docs.harness.io/article/3j87za7lho-open-shift-connected-on-prem-setup#installation">Installation</a>. </p>
    <p></p>
    <h6>Integrated OpenShift Container Registry (OCR)</h6>
    <p></p><pre>oc create secret -n harness docker-registry regcred --docker-server=&lt;repo_url&gt; --docker-username=unused --docker-password=&lt;password&gt;</pre>
    <p>The password used in creating the <strong>regcred</strong> secret should meet the following requirements:</p>
    <ol>
      <li>The password can be of a regular user and service account. </li>
      <li>The user or service account used to log into OCR should have the following roles:
        <ol>
          <li>registry-viewer</li>
          <li>registry-editor</li>
          <li>system:image-builder</li>
        </ol>
      </li>
    </ol>
    <p> As a recommendation, the registry credentials of the service account <code>harness-namespace-admin</code> can be used in the above command. To get these credentials, run this command:</p><pre>oc get secret -n harness $(oc get sa <strong>harness-namespace-admin</strong> -n harness -o json | jq -r &#39;.imagePullSecrets[0].name&#39;) -o json | jq -r &#39;.data.&#34;.dockercfg&#34;&#39; | base64 -d | jq -r &#39;[.[]][0].password&#39;</pre>
    <p>Copy the password from the output of the above command and use it to create the secret.</p>
    <p></p>
    <h6>Other Docker Repository</h6>
    <p></p><pre>oc create secret -n harness docker-registry regcred --docker-server=&lt;repo_url&gt; --docker-username=&lt;user&gt; --docker-password=&lt;password&gt;</pre>
    <h4>Load Balancer Setup</h4>
    <p>The load balancer must be preconfigured to connected to the Ingress controller in the <code>harness</code> namespace. Also, the internal URL must be provided to Harness as instructed in the Installation section.</p>
    <h3>Installation</h3>
    <p>Once the infrastructure is prepared (per the Prerequisites listed above), Harness installation in your OpenShift cluster involves the following steps:</p>
    <ol>
      <li>Email the following details to the Harness Team:
        <ol>
          <li>Repository URL (internal URL, used by the Ambassador and OpenShift cluster).</li>
          <li>Load Balancer URL (internal URL, used by the Ambassador and OpenShift cluster).</li>
          <li>Storage Class configured in the cluster.</li>
        </ol>
      </li>
      <li>Harness provides the script to run on the Ambassador. This script includes the signed URL that has access to download the binaries and start the Ambassador process. Once the process is completed, notify the <strong>Harness Team</strong> about the success
        of this step.</li>
      <li>After validating the connectivity of the Ambassador to the Cloud, the <strong>Harness Team</strong> triggers the installation of the platform on the OpenShift cluster, as configured.</li>
    </ol>
    <p></p>
    <div class="hd--md" data-hd-markdown="&lt;span id=&#34;verification&#34;&gt;&lt;/span&gt;">
      <p><span id="verification"></span></p>
    </div>
    <h3>Post-Installation Steps</h3>
    <ol>
      <li>After Harness completes the installation, log into the admin setup link:
        <br/> <code>
    https://LOAD_BALANCER_URL/#/onprem-signup
    </code></li>
      <li>In the resulting signup form, set up the initial admin account by entering the requested details.
        <div class="tip-callout">All subsequent logins will go to the standard URL: <code>https://LOAD_BALANCER_URL</code></div>
      </li>
      <li>Log into the account using the load-balancer URL: <code>https://LOAD_BALANCER_URL</code>
        <br/>
        <br/>The Harness On-Prem application should open successfully.</li>
    </ol>
    <p></p>
    <div class="hd--md" data-hd-markdown="&lt;span id=&#34;next_steps&#34;&gt;&lt;/span&gt;">
      <p><span id="next_steps"></span></p>
    </div>
    <h3>Next Steps</h3>
    <ol>
      <li>Install the Harness Delegate:
        <a href="/article/h9tkwmkrm7-delegate-installation">Delegate Installation and Management</a>.</li>
      <li>Set up an SMTP Collaboration Provider in Harness for email notifications from the Harness Manager:
        <a href="/article/cv98scx8pj-collaboration-providers">Add Collaboration Providers</a>.</li>
    </ol>
    <p></p>
  slug: open-shift-connected-on-prem-setup
  tags:
  - on-premise
  - OpenShift
  - kubeconfig
  - YAML
  - kubectl
  - Ambassador
  is_live: true
