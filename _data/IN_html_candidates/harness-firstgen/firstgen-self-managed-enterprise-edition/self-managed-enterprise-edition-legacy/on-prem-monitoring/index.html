<p>This topic outlines how to monitor the health of a <a href="/article/vf4cy95mst-harness-connected-on-premise-setup">Harness Docker Connected On-Prem</a> setup, in the following sections:</p><ul><li><a href="#overview">Overview</a></li><li><a href="#infrastructure_requirements">Infrastructure Requirements</a></li><li><a href="#prerequisites">Prerequisites</a></li><li><a href="#access_urls">Access URLs</a></li><li><a href="#dashboards">Dashboards</a></li><li><a href="#default_alert_setup">Default Alert Setup</a></li><li><a href="#respond_alerts">Responding to Alerts</a></li><li><a href="#customizing_alerts">Customizing Alerts</a></li><li><a href="#customizing_components">Customizing Prometheus and Grafana</a></li></ul><p></p><div class="hd--md" data-hd-markdown="&lt;span id=&#34;overview&#34;&gt;&lt;/span&gt;"><p><span id="overview"></span></p>
</div><h3>Overview</h3><p>A Harness Docker Connected On-Prem (three-box) installation runs various microservices on different hosts. You can monitor Harness microservices using the monitoring service provided as part of the Harness On-Prem installation. As outlined below, the monitoring service uses four components (which are themselves microservices):</p><p></p><p></p><figure><img src="https://files.helpdocs.io/kw8ldg1itf/articles/wd8ltzh855/1583190409159/image.png" style="max-height:100%;max-width:100%" data-hd-height="100%" data-hd-width="100%"/></figure><p><a href="https://github.com/google/cadvisor"><u>cAdvisor</u></a>: This will be installed on each host, and is used to scrape the host for the different metrics that Harness needs for monitoring. </p><p><a href="https://prometheus.io/"><u>Prometheus</u></a>: This serves as a persistent store for the metrics scraped by cAdvisor and by Prometheus&#39; own alert-triggering platform. Within Prometheus, you can set up different alert rules over different metrics received from cAdvisor. Prometheus keeps polling cAdvisor for different metrics, and keeps matching them with the configured alert rules.</p><p><a href="https://prometheus.io/docs/alerting/alertmanager/"><u>Alertmanager</u></a>: This propagates the alerts created by Prometheus to your notification platform (Slack, email, etc.). Prometheus creates alerts according to the alerts metrics it gets from cAdvisor, and passes the alerts to Alertmanager. As input, Alertmanager takes a configuration file that specifies how Alertmanager will connect to different notification platforms. </p><p><a href="https://grafana.com/"><u>Grafana</u></a><strong>:</strong> Grafana is a dashboarding platform that relies on Prometheus data. It creates various time-series dashboards for CPU and memory usage, network traffic, I/O, and other critical metrics. </p><p></p><div class="hd--md" data-hd-markdown="&lt;span id=&#34;infrastructure_requirements&#34;&gt;&lt;/span&gt;"><p><span id="infrastructure_requirements"></span></p>
</div><h3><strong>Infrastructure Requirements</strong></h3><p>Harness recommends deploying the monitoring service on a separate host from the three boxes that it monitors. Typically, this host can be the same machine on which the Ambassador is running. The cAdvisor, Prometheus, Grafana, and Alertmanager components are each configured to use .25 cores of CPU and 500 MB RAM.</p><p></p><div class="hd--md" data-hd-markdown="&lt;span id=&#34;prerequisites&#34;&gt;&lt;/span&gt;"><p><span id="prerequisites"></span></p>
</div><h3><strong>Prerequisites</strong></h3><p>If you are running RHEL7 OS on your hosts: RHEL 7 boxes have a default configuration that blocks cAdvisor from scraping system metrics. (For details, see <a href="https://github.com/google/cadvisor/issues/1843" target="_blank">this cAdvisor issue</a>.) Use the following procedure to enable cAdvisor to scrape metrics.</p><div class="note-callout"> You must execute these steps as a root user, on all three machines.</div><ol><li>On a command line, execute these commands to mount the <code>cgroup</code> directory:<pre class="hljs bash">mount -o remount,rw &#39;/sys/fs/cgroup&#39;<br/>ln -s /sys/fs/cgroup/cpu,cpuacct /sys/fs/cgroup/cpuacct,cpu</pre></li><li>Run the command to open the crontab:<pre class="hljs bash">crontab -e</pre></li><li>Append these two lines to the bottom of the crontab file, to make the remount commands persistent upon host restarts:<pre class="hljs bash">@reboot mount -o remount,rw &#39;/sys/fs/cgroup&#39;<br/>@reboot ln -s /sys/fs/cgroup/cpu,cpuacct /sys/fs/cgroup/cpuacct,cpu</pre></li><li>Save the crontab file.</li></ol><p></p><div class="hd--md" data-hd-markdown="&lt;span id=&#34;access_urls&#34;&gt;&lt;/span&gt;"><p><span id="access_urls"></span></p>
</div><h3><strong>Access URLs</strong></h3><p>The four monitoring components can be accessed at the following URLs:</p><ul><li><strong>cAdvisor:</strong> [runs on all hosts on port 7152]</li><li><strong>Prometheus:</strong> <code>http://monitoringhost​:7149/alerts</code></li><li><strong>Alertmanager:</strong> <code>http://monitoringhost:7150/</code></li><li><strong>Grafana:</strong> <code>http://monitoringhost:7152/</code></li></ul><p></p><div class="hd--md" data-hd-markdown="&lt;span id=&#34;dashboards&#34;&gt;&lt;/span&gt;"><p><span id="dashboards"></span></p>
</div><h3><strong>Dashboards</strong></h3><p>Harness Monitoring installs a preconfigured Grafana setup, which can be used to monitor containers&#39; health, along with other metrics like CPU and memory usage, network traffic, and I/O. The Grafana dashboard provides a combined graphical view of container metrics per box.</p><p>To add a basic set of graphs to Grafana, download the JSON template from <a href="https://grafana.com/grafana/dashboards/3125">this Docker monitoring dashboard</a>, and import it in Grafana. This will create a dashboard like the one shown below.</p><p></p><figure><img src="https://files.helpdocs.io/kw8ldg1itf/articles/wd8ltzh855/1582359570592/image.png" style="max-height:100%;max-width:100%" data-hd-height="100%" data-hd-width="100%"/></figure><p>Grafana provides advanced features like templatizing dashboards, changing dashboards&#39; granularity, built-in authentication, and adding custom dashboards to match your requirements. For details about adding dashboards, see <a href="https://grafana.com/docs/grafana/latest/">Grafana&#39;s documentation</a>.</p><p></p><div class="hd--md" data-hd-markdown="&lt;span id=&#34;default_alert_setup&#34;&gt;&lt;/span&gt;"><p><span id="default_alert_setup"></span></p>
</div><h3><strong>Default Alert Setup</strong></h3><p>As shipped, Harness&#39; monitoring service provides the four above components, along with the following configuration:</p><ol><li>Alerts are configured on all the Harness microservices (Manager, UI, Verification Service, Learning Engine, etc.) running on the three hosts. If Prometheus doesn’t receive any heartbeat from the microservices for 5 minutes, it triggers an alert, which is sent to the configured notification method.</li><li>All alerts are grouped by the host on which they are running. Once a grouped alert is sent for one or multiple microservices, the alert is repeated every 10 minutes. You can change this value in the Alertmanager configuration. This is how one grouped alert looks in email:<figure><img src="https://files.helpdocs.io/kw8ldg1itf/articles/wd8ltzh855/1582359636367/image.png" style="max-height:75%;max-width:75%" data-hd-height="75%" data-hd-width="75%"/></figure></li><li>If any container goes down, the first alert is sent after 5 minutes.</li><li>If the Alertmanager container goes down for any reason, any changes you have made in Alertmanager during the preceding 10 minutes will be lost. (Alertmanager writes data to disk on a 10-minute lag.)</li></ol><p></p><div class="hd--md" data-hd-markdown="&lt;span id=&#34;respond_alerts&#34;&gt;&lt;/span&gt;"><p><span id="respond_alerts"></span></p>
</div><h3>Responding to Alerts</h3><p>Here are recommendations for responding to some common alert conditions.</p><h4>All Microservices on a Box Are Down</h4><p>This can happen if the box restarted, or if the Docker daemon on the box restarted. Use the Harness local <strong>start script</strong> to bring the system back to its healthy state.</p><h4>A Few Microservices on a Box Are Down</h4><p>Look at the logs of the container that has gone down. These should give some indication of why the service is down. For example, if <code>mongoContainer</code> is down, you can use the command: <code>docker inspect mongoContainer</code></p><p>In the case of <code>harnessManager</code>, the logs will be present in:</p><p><code>&lt;HOME_DIR&gt;/Connected on-prem docker installer/Manager/&lt;CUSTOMER_NAME&gt;/runtime/logs/portal.log</code></p><p>If the service is down because of an infrastructure issue (such as out of disk space), correct the infrastructure issue, and then use Harness&#39; local <strong>start script</strong> to restore Harness services.</p><p>If the service is down because of some other issue, contact Harness Support at <a href="mailto:support@harness.io">support@harness.io</a>. Specify the issue you&#39;re seeing, and include the container logs. Harness Support will analyze the problem, fix it, and roll out an update using the Ambassador.</p><p></p><div class="hd--md" data-hd-markdown="&lt;span id=&#34;customizing_alerts&#34;&gt;&lt;/span&gt;"><p><span id="customizing_alerts"></span></p>
</div><h3>Customizing Alerts</h3><p>You can customize the following aspects of Harness&#39; monitoring service.</p><h4>Set Up Custom Alert Rules</h4><p>The Prometheus microservice controls alerts. To see existing alert rules, go to Prometheus and access Status &gt; Alerts. One alert rule looks something like this:</p><pre>  - alert: Manager_HOST1<br/>    expr: absent(container_last_seen{name=&#34;harnessManager&#34;, instance=&#34;HOST_1:7152&#34;})<br/>    for: 1m<br/>    labels:<br/>      severity: error<br/>    annotations:<br/>      summary: &#34;Harness container is down on HOST_1&#34;<br/>      description: &#34;harnessManager is down on: HOST_1&#34;</pre><p>All alert rules reside in the <code>~/$INSTALLER_DIR/Prometheus/&lt;CUSTOMER_NAME&gt;/runtime/alert.rules</code> file. To add new rules, append them to the end of this file, then restart Prometheus. For alert configuration details, see Prometheus&#39; <a href="https://prometheus.io/docs/alerting/configuration/"><u>Alerting Configuration</u></a> documentation.</p><h4>Set Up Custom Alert Notification Platforms</h4><p>The Alertmanager microservice handles alert delivery. Alertmanager supports multiple integrations, including email and Slack. Alertmanager’s default configuration provides email notifications, and resides in the <code>~/$INSTALLER_DIR/Prometheus/&lt;CUSTOMER_NAME&gt;/runtime/alertmanager.yaml</code> file. For details on adding custom notification providers to this configuration file, refer back to Prometheus&#39; <a href="https://prometheus.io/docs/alerting/configuration/"><u>Alerting Configuration</u></a> documentation.</p><h4>Set Up Custom Alert Notification Frequency</h4><p>You can configure the alerting frequency and grouping in this file:</p><p> <code>~/Connected on-prem docker installer/Alert Manager/&lt;CUSTOMER_NAME&gt;/runtime/alertmanager.yaml</code></p><p>In the file’s <code>route</code> section, you must update the following settings:</p><pre>  # When a new group of alerts is created by an incoming alert, wait at<br/>  # least &#39;group_wait&#39; to send the initial notification.<br/>  # This way ensures that you get multiple alerts for the same group that start<br/>  # firing shortly after another are batched together on the first<br/>  # notification.<br/>  group_wait: 30s<br/><br/>  # When the first notification was sent, wait &#39;group_interval&#39; to send a batch<br/>  # of new alerts that started firing for that group.<br/>  group_interval: 5m<br/><br/>  # If an alert has successfully been sent, wait &#39;repeat_interval&#39; to<br/>  # resend them.<br/>  repeat_interval: 3h</pre><h4>Restart Prometheus to Apply Configuration Updates</h4><p>For any configuration update to take effect, you must restart the Prometheus container.</p><p>If you have updated alert.rules, run following command to restart Prometheus:</p><pre class="hljs bash">docker restart harness-prometheus</pre><p>If you have updated alertmanager.yaml, run following command to restart Alertmanager:</p><pre class="hljs bash">docker restart harness-alertmanager</pre><p></p><div class="hd--md" data-hd-markdown="&lt;span id=&#34;customizing_components&#34;&gt;&lt;/span&gt;"><p><span id="customizing_components"></span></p>
</div><h3>Customizing Prometheus and Grafana</h3><p>If you have a preconfigured Prometheus and Grafana system, you can reconfigure these components to point to the three hosts running Harness microservices. Prometheus can scrape metrics from cAdvisor on each of the three hosts, on port 7152.</p><p></p><div class="hd--md" data-hd-markdown="&lt;span id=&#34;configure_prometheus&#34;&gt;&lt;/span&gt;"><p><span id="configure_prometheus"></span></p>
</div><h4>Configure Custom Prometheus</h4><p>To customize Prometheus according to your needs, update the <code>prometheus.yml</code> file. For details, see Prometheus&#39; <a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/" target="_blank">Configuration</a> documentation.</p><p>Below is Harness&#39; recommended <code>prometheus.yml</code> configuration. Its <code>scrape_configs</code> section shows how to configure Prometheus to get information from cAdvisors on the three boxes.</p><p></p><pre>  # my global config<br/>  global:<br/>    scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.<br/>    evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.<br/>    # scrape_timeout is set to the global default (10s).<br/><br/>    external_labels:<br/>        monitor: &#39;harness-monitor&#39;<br/><br/>  # Load rules once and periodically evaluate them according to the global &#39;evaluation_interval&#39;.<br/>  rule_files:<br/>    - /etc/prometheus/alert.rules<br/>    # - &#34;second.rules&#34;<br/><br/>  # A scrape configuration containing exactly one endpoint to scrape:<br/>  # Here it&#39;s Prometheus itself.<br/>  scrape_configs:<br/>    # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.<br/>    - job_name: &#39;prometheus&#39;<br/><br/>      # metrics_path defaults to &#39;/metrics&#39;<br/>      # scheme defaults to &#39;http&#39;.<br/><br/>      static_configs:         <br/>        - targets: [&#39;&lt;MONITORING_HOST&gt;:7149&#39;]<br/><br/>    - job_name: &#39;cadvisor-metrics&#39;<br/>           # metrics_path defaults to &#39;/metrics&#39;<br/>           # scheme defaults to &#39;http&#39;.<br/><br/>      static_configs:<br/>        - targets: [&#39;&lt;HOST_1&gt;:7152&#39;, &#39;&lt;HOST_2&gt;:7152&#39;, &#39;&lt;HOST_3&gt;:7152&#39;]<br/><br/>  alerting:<br/>    alertmanagers:<br/>      - static_configs:<br/>        - targets: [&#39;&lt;MONITORING_HOST&gt;:7150&#39;]<br/></pre><p></p><div class="hd--md" data-hd-markdown="&lt;span id=&#34;configure_grafana&#34;&gt;&lt;/span&gt;"><p><span id="configure_grafana"></span></p>
</div><h4>Configure Custom Grafana</h4><p>For details on configuring Grafana, see Prometheus&#39; <a href="https://prometheus.io/docs/visualization/grafana/" target="_blank">Grafana Support for Prometheus</a> documentation.</p>