type: article
article_id: jzklic4y2j
user_id: mfr0nxh4be
category_id: svxv7bn8as
author:
  name: Michael Cretzman
  profile_image: https://www.gravatar.com/avatar/2e8616837f4ee92be5d19ffe9b9ccba9?d=mm&s=150
title: Troubleshooting Harness
slug: troubleshooting
description: Harness error messages, causes, and solutions.
short_version: Harness error messages, causes, and solutions.
tags: []
show_toc: true
is_private: false
is_published: true
is_featured: false
stale_status:
  is_stale: false
  reason: ""
  source: API
  triggered_at: 2022-07-07T22:54:30.317419Z
  expires_at: null
permission_groups: []
multilingual:
- language_code: en
  title: Troubleshooting Harness
  description: Harness error messages, causes, and solutions.
  short_version: Harness error messages, causes, and solutions.
  body: '<p>This topic contains general troubleshooting information for error messages
    and other issues that can arise.</p><div class="note-callout">If you cannot find
    a resolution, please contact <a href="mailto:support@harness.io">Harness Support</a> or <a
    href="https://community.harness.io/" target="_blank">Harness Community Forum</a>.</div><h3>Login
    Issues</h3><p>The following issues can occur when logging into Harness.</p><h4>Logged
    Out Automatically</h4><p>You are logged out of your Harness Manager session automatically,
    forcing you to log back in.</p><div class="note-callout">If you log out of Harness
    Manager in one browser tab, Harness might log you out of all tabs.</div><p>Typically,
    the solution is to clear local storage.</p><h5>Troubleshooting Steps</h5><ol><li>Log
    out of the Harness Manager from all Chrome tabs. (Harness only supports the Chrome
    desktop browser.)</li><li>Clear Chrome Local Storage for <code>app.harness.io</code>
    in <strong>chrome://settings/siteData</strong>.</li><li>Open a new tab and log
    into the Harness Manager.</li></ol><p>You should not be logged out anymore.</p><h5>Notes</h5><ul><li>Chrome
    <a href="https://developers.google.com/web/tools/chrome-devtools/storage/sessionstorage"
    target="_blank">Session Storage</a> is used by the Harness Manager. If you close
    all the tabs running the Harness Manager and then open a new tab running Harness
    Manager, you will likely need to log in again.</li><li>A Chrome session will timeout
    after 5 minutes, but a session timeout can also happen if the tab running Harness
    Manager is idle for 24 hours. However, as long as the tab is not closed, Harness
    Manager will continue keep polling to check if a refresh is needed for the token.
    For example, if you have kept the tab open for 3 days, you might still be logged
    in, as long as the workstation has not been turned off or entered sleep mode preventing
    the refresh.</li></ul><p></p><h3>Delegate Issues</h3><p>Harness Delegates run
    as a service in your deployment or build farm environment, on a host, a pod, a
    container, or as a task. They make outbound HTTPS connections over port 443 and
    use the credentials you provide in Harness Connectors such as Cloud Providers
    and Artifact Servers to run remote SSH and API calls.</p><p>See <a href="/article/2k7lnc7lvl-delegates-overview">Delegates
    Overview</a>.</p><p>Most Delegate issues arise from network connectivity where
    the Delegate is unable to connect to a cloud provider, artifact server, etc, because
    of network issues like port changes and proxy settings.</p><p>Some issues arise
    from invalid credentials due to expiry or access issues resulting from missing
    policies or cross project requirements in a cloud vendor.</p><p>The simplest way
    to detect if an issue is caused by Delegate connectivity is to run a cURL command
    on the Delegate host/pod and see if it works. If it does, the next step is to
    look at the credentials.</p><p>The following sections provide solutions to Delegate
    issues.</p><h4>Failed to assign any Delegate to perpetual task</h4><p>Harness
    does many background operations on a regular basis, such as collecting information
    about your cluster and deployed software. This ensures that the number of instances
    we report is correct, among other information.</p><p>This error message is related
    to these background operations. Subsequent, scheduled attempts typically clears
    these messages.</p><p>If these errors clear, typically a local or remote networking
    or similar issue is the cause.</p><h4>Duplicate Output in Deployment Logs</h4><p>This
    is a symptom of running duplicate Delegates. We call this the double Delegate
    problem.</p><p>If two Harness Delegates with the same name are running in different
    clusters, they will show up as one Delegate in the Harness Manager. This will
    make it seem as though only one Delegate is running.</p><p><strong>Do not run
    Delegates with the same name in different clusters.</strong> Replace one of the
    Delegates and the issue will go away.</p><p>You might see errors such as <code>IllegalArgumentException</code>
    and multiple <code>Initializing</code> and <code>Rendering</code> lines:</p><pre>Initializing..<br/><br/>Rendering
    manifest files using go template<br/>Only manifest files with [.yaml] or [.yml]
    extension will be processed<br/><br/>Initializing..<br/><br/>Rendering manifest
    files using go template<br/><br/>Only manifest files with [.yaml] or [.yml] extension
    will be processed<br/><br/>IllegalArgumentException: Custom Resource Definition
    Optional[destinationrules.networking.istio.io] is not found in cluster https://0.00.0.1:443/<br/><br/>Failed.</pre><p></p><h4>Running
    Multiple Delegates on the Same Host</h4><p>If deployment entities are getting
    added and removed in the same deployment, you might have two Delegates running
    on the same host.</p><p>Do not run multiple Delegates on the same host/pod/container.
    This will result in the Delegates overwriting each other&#39;s tasks.</p><h4>Delegate
    Setup</h4><p>Most often, Delegate errors are the result of Delegate setup issues.
    Ensure you are familiar with how the Delegate and Harness Manager work together.
    See <a href="/article/re8kk0ex4k-delegate-installation-overview">Delegate Installation
    Overview</a>.</p><p>Another common issue is the SSH key used by the Delegate to
    deploy to a target host is incorrect. This can happen if the SSH key in <a href="/article/hngrlb7rd6-harness-secret-manager-overview">Harness
    Secrets Management</a> was set up incorrectly, or if it is not the correct key
    for the target host, or the target host is not set up to allow SSH connections.</p><p>The
    Delegate is monitored locally using its Watcher component. The Watcher component
    has a watcher.log file that can provide Delete version information for troubleshooting.</p><h4>Delegate
    Connection Failures To Harness Manager</h4><p>If the Delegate cannot connect to
    the Harness Manager, try the following:</p><ol><li>Use <strong>ping</strong> on
    the Delegate host to test if response times for <strong>app.harness.io</strong>
    or another URL are reasonable and consistent.</li><li>Use <strong>traceroute</strong>
    on <strong>app.harness.io</strong> to check the network route.</li><li>Use <strong>nslookup</strong>
    to confirm that DNS resolution is working for <strong>app.harness.io</strong>.</li><li>Connect
    using the IP address for <strong>app.harness.io</strong> (get the IP address using
    nslookup), for example: <code>http://35.23.123.321/#/login</code>.</li><li>Flush
    the client&#39;s DNS cache<ol><li>Windows: <code>ipconfig /flushdns</code></li><li>Mac/Linux:
    <code>sudo killall -HUP mDNSResponder;sudo killall mDNSResponderHelper;sudo dscacheutil
    -flushcache</code></li></ol></li><li>Check for local network issues, such as proxy
    errors or NAT license limits.</li><li>For some cloud platforms, like AWS EC2,
    ensure that security groups allow outbound traffic on HTTPS 443.</li><li>Try a
    different workstation or a smartphone to confirm the connection issue is not local
    to a single host.</li></ol><h4>Delegate Successes Followed By Failures</h4><p>If
    you have incorrectly used the same Kubernetes Delegate YAML file for multiple
    Delegates, you will see Delegate successes followed by failures in the Delegate
    logs. This sequence is the result of one Delegate succeeding in its operation
    and the same operation failing with the second Delegate.</p><div class="note-callout">To
    avoid any Delegate conflicts, always use a new Kubernetes Delegate YAML download
    for each Delegate you install, and a unique name.</div><p>For Kubernetes Delegates,
    you can increase the number of replicas run using a single Delegate download YAML
    file (change the <code>replicas</code> setting in the file), but to run multiple
    Delegates, use a new Delegate download from Harness for each Delegate.</p><h4>No
    Delegates Could Reach The Resource</h4><p>This error means that no Delegate could
    meet the URL criteria for validation. For more information, see <a href="/article/2k7lnc7lvl-delegates-overview#how_does_harness_manager_pick_delegates">How
    Does Harness Manager Pick Delegates?</a>.</p><h4>Google Cloud Platform: Cluster
    has unschedulable pods</h4><p>If you do not have enough space available in your
    Kubernetes cluster, you might receive the following error:</p><figure><img src="https://files.helpdocs.io/kw8ldg1itf/articles/h9tkwmkrm7/1542232002879/image.png"
    style="max-height:50%;max-width:50%" data-hd-height="50%" data-hd-width="50%"/></figure><h5>Cause</h5><p>Depending
    on the size of your cluster, without Autoscaling enabled or enough space, your
    cluster cannot run the delegate.</p><h5>Solution</h5><p>Add more space or turn
    on Autoscaling, wait for the cluster to restart, reconnect to the cluster, and
    then rerun the command:</p><p><code>$ kubectl apply -f harness-delegate.yaml</code></p><p>For
    more information, see <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/scaling-apps#autoscaling_deployments"
    target="_blank">Autoscaling Deployments</a> from Google.</p><h4>Deleting a Kubernetes
    Delegate</h4><p>In the case where you have to delete a Harness Delegate from your
    Kubernetes cluster, you can delete the StatefulSet for the Delegate.</p><p>Once
    created, the StatefulSet ensures that the desired number of pods are running and
    available at all times. Deleting the pod without deleting the StatefulSet will
    result in the pod being recreated.</p><p>For example, if you have the Delegate
    pod name <code>mydelegate-vutpmk-0</code>, you can delete the StatefulSet with
    the following command:</p><p><code>$ kubectl delete statefulset -n harness-delegate
    mydelegate-vutpmk</code></p><p>Note that the <code>-0</code> suffix in the pod
    name is removed for the StatefulSet name.</p><h4>Self Destruct Sequence Initiated</h4><p>This
    very rare error can be noticed in Delegate logs:</p><pre class="hljs nohighlight">Sending
    heartbeat...<br/><br/>Delegate 0000 received heartbeat response 0s after sending.
    26s since last response.<br/><br/>Self destruct sequence initiated...</pre><h5>Cause</h5><p>Delegate
    self-destructing because are two Delegates with same name, probably deployed to
    two different clusters.</p><h5>Solution</h5><p>Remove one Delegate. Typically,
    one Delegate is in the wrong cluster. Remove that Delegate.</p><h4>Need to Use
    Long Polling for Delegate Connection to Harness Manager</h4><p>By default, the
    Harness Delegate connects to the Harness Manager over a TLS-backed WebSocket connection,
    sometimes called a Secure WebSocket connection, using the <code>wss://</code>
    scheme (<a href="https://tools.ietf.org/html/rfc6455#section-11.1.2">RFC 6455</a>).</p><p>Some
    network intermediaries, such as transparent proxy servers and firewalls that are
    unaware of WebSocket, might drop the WebSocket connection. To avoid this uncommon
    error, you can instruct the Delegate to use long polling.</p><p>To set up the
    Delegate to use long polling, you use the Delegate YAML file.</p><p>For a Kubernetes
    Delegate, you can set the <code>POLL_FOR_TASKS</code> setting to <code>true</code>
    in the <strong>harness-delegate.yaml</strong> file:</p><pre>...<br/>        env:<br/>        ...<br/>        -
    name: POLL_FOR_TASKS<br/>          value: &#34;true&#34;<br/>...</pre><p></p><h4>KubernetesClientException:
    Operation: [list] for kind: [Deployment] with name: [null] in namespace: [default]
    failed</h4><p>If you have a proxy set up on the network where the Harness Kubernetes
    Delegate is running, you need to add the cluster master hostname or IP in the
    Delegate harness-delegate.yaml <code>NO_PROXY</code> list.</p><p>For example,
    you might see a log error like this:</p><pre>io.fabric8.kubernetes.client.KubernetesClientException:
    Operation: [list]  for kind: [Deployment]  with name: [null]  in namespace: [default]  failed.</pre><ol><li>Obtain
    the cluster master hostname or IP (<code>kubectl cluster-info</code>).</li><li>Open
    the <strong>harness-delegate.yaml</strong> you used to create the Delegate, and
    add the cluster master hostname or IP to the <code>NO_PROXY</code> setting in
    the <code>StatefulSet</code> spec:</li></ol><pre>        - name: NO_PROXY<br/>          value:
    &#34;192.0.2.0&#34;</pre><ol><li style="counter-increment:li 2" start="3">Apply
    harness-delegate.yaml again to restart the Kubernetes Delegate (<code>kubectl
    apply -f harness-delegate.yaml</code>).</li></ol><p></p><h3>Artifact Collection</h3><p>This
    section lists common errors you might receive when Harness attempts to collect
    artifacts.</p><h4>Stage Hanging on Artifact Collection</h4><p>If a Delegate has
    been offline for an extended period of time, you might need to reset the Harness
    Connector credentials.</p><p></p><h3>Common Errors and Alerts</h3><p>This section
    lists common error and alert messages you might receive.</p><h4 id="no_delegates_could_reach_the_resource">No
    Delegates Could Reach The Resource</h4><p>This error means that no Delegate could
    meet the URL criteria for validation. When a task is ready to be assigned, the
    Harness Manager first validates its lists of Delegates to see which Delegate should
    be assigned the task. It validates the Delegate using the URL in the task, such
    as a API call or SSH command. See  <a href="/article/2k7lnc7lvl-delegates-overview#how_does_harness_manager_pick_delegates">How
    Does Harness Manager Pick Delegates?</a>.</p><h4>Harness SecretStore Is Not Able
    to Encrypt/Decrypt</h4><p>Error message:</p><pre>Secret manager Harness SecretStore
    of type KMS is not able to encrypt/decrypt. Please check your setup</pre><p></p><p>This
    error results when Harness Secret Manager (named <strong>Harness SecretStore</strong>)
    is not able to encrypt or decrypt keys stored in AWS KMS. The error is usually
    transitory and is caused by a network connectivity issue or brief service outage.</p><p>Check
    <a href="https://status.harness.io/" target="_blank">Harness Site Status</a> and
    <a href="https://status.aws.amazon.com/">AWS Status</a> (search for <strong>AWS
    Key Management Service</strong>).</p><h4>You are not authorized to perform this
    operation: AmazonEC2: Status code 403</h4><p>This error occurs when you are testing
    a Harness AWS Connector and the credentials used for the connection do not include
    a policy with the <a href="https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeRegions.html"
    target="_blank">DescribeRegions</a> action.</p><p><strong>The DescribeRegions action
    is required for all AWS Connectors.</strong> Harness tests the connection using
    an API call for the DescribeRegions action.</p><p>This is described in <a href="/article/98ezfwox9u-add-aws-connector">Add
    an AWS Connector</a>.</p><p>Ensure that one of the IAM roles assigned to the user
    account used for AWS Connector credentials contains the <u>DescribeRegions</u> action.</p><h4>Git-upload-pack
    not permitted</h4><p>One possible cause of this error is if you are using a Personal
    Access Token (PAT) for your GitHub Connector and your GitHub organization uses
    SAML single sign-on (SSO).</p><p>To use a personal access token with a GitHub
    organization that uses SAML single sign-on (SSO), you must first authorize the
    token. See <a href="https://docs.github.com/en/enterprise-cloud@latest/authentication/authenticating-with-saml-single-sign-on/authorizing-a-personal-access-token-for-use-with-saml-single-sign-on"
    target="_blank">Authorizing a personal access token for use with SAML single sign-on</a> from
    GitHub.</p><pre>org.eclipse.jgit.api.errors.TransportException: https://github.com/*******/*******:
    git-upload-pack not permitted on &#39;https://github.com/*******/*******/&#39;</pre><p></p><h3>Naming
    Conventions</h3><div class="note-callout">Typically, names for Harness entities
    can only contain alphanumerics, _ and -.</div><p>Some naming conventions in repositories
    and other artifact sources, or in target infrastructures, cannot be used by Harness.
    For example, if a Harness Trigger Webhook uses a Push notification from a Git
    repo branch that contains a dot in its name, the Trigger is unlikely to work.</p><p>Character
    support in Harness Environment and Infrastructure Definition entity names is restricted
    to alphanumeric characters, underlines, and hyphens. The restriction is due to
    compatibility issues with Harness backend components, database keys, and the YAML
    flow where Harness creates files with entity names on file systems.</p><h3>Secrets</h3><p>The
    following issues can occur when using Harness secrets.</p><h4>Secrets Values Hidden
    In Log Output</h4><p>If a secret&#39;s unencrypted value shares some content with
    the value of another Harness variable, Harness will hide the secret&#39;s value
    in any logs. Harness replaces the secret&#39;s conflicting value with the secret&#39;s
    name in any log displays. This is for security only, and the actual value of the
    secrets and variables are still substituted correctly.</p><h4>AWS KMS 403</h4><p>The
    Harness Delegate runs in your target deployment environment and needs access to
    the default Harness AWS KMS for secrets management. If it does not have access,
    the following error can occur:</p><pre>Service: AWSKMS; Status Code: 403</pre><p>Ensure
    that the Delegate can reach the Harness KMS URL by logging into the Delegate host(s)
    and entering the following cURL command:</p><pre>curl https://kms.us-east-1.amazonaws.com</pre><p>Next,
    ensure that your proxies are not blocking the URL or port 443.</p><p>If this does
    not fix your error, and you are not using the default Harness KMS secret store,
    the AWS KMS access key provided in Harness for your own KMS store is likely invalid.</p><h3>Triggers</h3><p>This
    section covers error messages you might see when creating, updating, deleting,
    or executing a Trigger. It includes authorization/permissions steps to resolve
    the errors.</p><h4>zsh: no matches found</h4><p>If you are using MacOS Catalina
    the default shell is zsh. The zsh shell requires that you escape the ? character
    in your cURL command or put quotes around the URL.</p><p>For example, this will
    fail:</p><pre>curl -X POST -H &#39;content-type: application/json&#39; --url https://app.harness.io/gateway/api/webhooks/xxx?accountId=xxx
    -d &#39;{&#34;application&#34;:&#34;fCLnFhwsTryU-HEdKDVZ1g&#34;,&#34;parameters&#34;:{&#34;Environment&#34;:&#34;K8sv2&#34;,&#34;test&#34;:&#34;foo&#34;}}&#39;</pre><p>This
    will work:</p><pre>curl -X POST -H &#39;content-type: application/json&#39; --url
    &#34;https://app.harness.io/gateway/api/webhooks/xxx?accountId=xxx -d &#39;{&#34;application&#34;:&#34;fCLnFhwsTryU-HEdKDVZ1g&#34;,&#34;parameters&#34;:{&#34;Environment&#34;:&#34;K8sv2&#34;,&#34;test&#34;:&#34;foo&#34;}}&#39;&#34;</pre><p></p><h4>User
    does not have &#34;Deployment: execute&#34; permission</h4><p>Error messages of
    the form <code>User does not have &#34;Deployment: execute&#34; permission</code>
    indicate that your user group&#39;s <strong>Role</strong> settings do not include
    <strong>Pipeline:</strong> <strong>Execute</strong>.</p><p></p><figure><img src="https://files.helpdocs.io/i5nl071jo5/articles/jzklic4y2j/1635202490707/clean-shot-2021-10-25-at-15-54-34.png"/></figure><p>To
    resolve this, see <a href="/article/tsons9mu0v-add-manage-roles">Add and Manage
    Roles</a>.</p><h3>Continuous Delivery</h3><p>The following issues can occur when
    running Pipeline deployments.</p><h4>Deployment Rate Limits</h4><p>If you&#39;ve
    reached 85% of the limit, you will see:</p><pre class="hljs nohighlight">85% of
    deployment rate limit reached. Some deployments may not be allowed. Please contact
    Harness support.</pre><p></p><p>If you&#39;ve reached 100% of the limit, you might
    see:</p><pre class="hljs nohighlight">Deployment rate limit reached. Some deployments
    may not be allowed. Please contact Harness support.</pre><p></p><p>Harness applies
    an hourly and daily deployment limit to each account to prevent configuration
    errors or external triggers from initiating too many undesired deployments. If
    you are notified that you have reached a limit, it is possible that undesired
    deployments are occurring. Please determine if a Trigger or other mechanism is
    initiating undesired deployments. If you continue to experience issues, contact
    <a href="mailto:support@harness.io" target="_blank">Harness Support</a>.</p><div
    class="note-callout">The daily limit is 100 deployments every 24 hours. The hourly
    limit is 40 deployments and is designed to detect any atypical upsurge of deployments.</div><h4>Error
    in Log When There is No Error</h4><p>When Harness captures commands output, it
    captures both standard out (stdout) and standard error (stderr) to the screen.
    Information from stdout receives the prefix <code>INFO</code> while information
    from stderr receives the prefix <code>ERROR</code>. This is meant to allow our
    users to know where the information they see comes from.</p><p>Unfortunately,
    several Linux commands and applications use standard out as a way to print information
    to the screen that will not be captured if the output is captured to a file.</p><p>For
    example, the cURL command shows a download progress indication on the screen.
    If you redirect cURL output to a file the progress indicator is not captured in
    the file. This is done by showing the progress indicator in standard out. This
    is a very useful feature for many users, but for Harness is causes the progress
    indicator to be seen with the <code>ERROR</code> prefix.</p><p>You can test this
    for yourself with the following short example:</p><pre>curl https://app.harness.io/version.txt
    &gt;out.txt 2&gt;err.txt<br/><br/>cat out.txt<br/><br/>cat err.txt</pre><p>As
    you can see, the err.txt file has the cURL command output that in Harness will
    show with the <code>ERROR</code> prefix.</p><p>If Harness does not show standard
    error, then many errors will not be captured, confusing customers. Therefore,
    Harness shows the standard error in its logs.</p><h3>Continuous Integration</h3><p>The
    following issues can occur when using CI into Harness.</p><h4>Test suites wrongly
    parsed</h4><p>The parsed Test report in the Test tab comes strictly from the JUnit
    reports provided. It is important to adhere to the standard format to improve
    test suite parsing.</p><p>Refer to the standard <a href="https://llg.cubic.org/docs/junit/">JUnit
    format</a>.</p><h4>Test Intelligence Not Working</h4><p>Test Intelligence may
    not work even after you select the <strong>Run only selected tests</strong> option.
    One of the reasons could be that you&#39;re using <strong>Maven</strong> and your
    <code><strong>pom.xml</strong></code> contains <code>argLine</code>. In such a
    case, the Java Agent must be updated as follows:</p><p>Before:</p><pre>&lt;argLine&gt;
    something<br/>&lt;/argLine&gt;</pre><p></p><p>After:</p><pre>&lt;argLine&gt; something
    -javaagent:/addon/bin/java-agent.jar=/addon/tmp/config.ini<br/>&lt;/argLine&gt;</pre><h3>Helm</h3><p>The
    following troubleshooting information should help you diagnose common Helm problems.</p><h4>Unable
    to get an Update from the Chart Repository</h4><p>If Harness cannot get an update
    from a chart repo you have set up for your Helm Service, during deployment you
    might see the error:</p><pre>Unable to get an update from the &#34;XYZ&#34; chart
    repository ... read: connection reset by peer</pre><p></p><p>To fix this, find
    the Delegate that the Helm update ran on, and then SSH to the Delegate host and
    run the Helm commands manually. This will confirm if you are having an issue with
    your Harness setup or a general connectivity issue.</p><h3>Kubernetes</h3><p>The
    following problems can occur when developing and deploying to Kubernetes.</p><h4>The
    Deployment is invalid...may not be specified when `value` is not empty</h4><p>Every
    Harness deployment creates a new release with an incrementally increasing number.
    Release history is stored in the Kubernetes cluster in a ConfigMap. This ConfigMap
    is essential for release tracking, versioning and rollback.</p><p>See <a href="/article/zahb65jgmy-kubernetes-releases-and-versioning">Kubernetes
    Releases and Versioning</a>.</p><p>If the ConfigMap is edited using kubectl or
    another tool between deployments future deployments often fail.</p><p>This type
    of error is experienced in standard Kubernetes deployments when attempting to
    use <code>kubectl apply</code> on a manifest whose resources have been previously
    modified using <code>kubectl edit</code>. For example, see the comments in this
    <a href="https://github.com/kubernetes/kubernetes/issues/78607" target="_blank">Kubernetes
    issue</a>.</p><h4>NullPointerException: Release Name is Reserved for Internal
    Harness ConfigMap</h4><p>The release name you enter in the Infrastructure Definition
    <strong>Release name</strong> is reserved for the internal Harness ConfigMap used
    for tracking the deployment.</p><p><strong>Do not create a ConfigMap that uses
    the same name as the release name.</strong> Your ConfigMap will override the Harness
    internal ConfigMap and cause a NullPointerException.</p><p>See <a href="/article/0ud2ut4vt2-define-your-kubernetes-target-infrastructure">Define
    Your Kubernetes Target Infrastructure</a>.</p><h4>The server doesn&#39;t have
    a resource type &#34;deployments&#34;</h4><p>When you attempt to connect to the
    Kubernetes cluster via <strong>GCP</strong>, the Kubernetes cluster must have
    <strong>Basic authentication enabled</strong> or the connection will fail. For
    more information, see <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/security-overview#control_plane_security">Control
    plane security</a> from GCP. From GCP:</p><p>You can handle cluster authentication
    in Google Kubernetes Engine by using Cloud IAM as the identity provider. However,
    legacy username-and-password-based authentication is enabled by default in Google
    Kubernetes Engine. For enhanced authentication security, you should ensure that
    you have disabled Basic Authentication by setting an empty username and password
    for the MasterAuth configuration. In the same configuration, you can also disable
    the client certificate which ensures that you have one less key to think about
    when locking down access to your cluster.</p><ul><li>If Basic authentication is
    inadequate for your security requirements, use the <a href="/article/1gaud2efd4-add-a-kubernetes-cluster-connector">Kubernetes
    Cluster Connector</a>.</li><li>While it can be easier to use the <a href="/article/1gaud2efd4-add-a-kubernetes-cluster-connector">Kubernetes
    Cluster Connector</a> for Kubernetes cluster deployments, to use a Kubernetes
    cluster on Google GKE, Harness requires a combination of Basic Authentication
    and/or Client Certificate to be enabled on the cluster:</li></ul><figure><img
    src="https://files.helpdocs.io/kw8ldg1itf/articles/g9o2g5jbye/1617833880390/image.png"/></figure><p></p><p>This
    is required because some API classes, such as the <a href="https://developers.google.com/resources/api-libraries/documentation/container/v1/java/latest/com/google/api/services/container/model/MasterAuth.html"
    target="_blank">MasterAuth class</a>, require HTTP basic authentication or client
    certificates.</p><h4 id="invalid_value_label_selector">Invalid Value LabelSelector</h4><p>If
    you are deploying different Harness Pipelines to the same cluster during testing
    or experimentation, you might encounter a Selector error such as this:</p><pre
    class="hljs css">The Deployment “harness-example-deployment” is invalid: spec.selector:
    <br/>  Invalid value: v1.LabelSelector{MatchLabels:map[string]string{“app”:“harness-example”},
    <br/>  MatchExpressions:[]v1.LabelSelectorRequirement{}}: field is immutable</pre><p>This
    error means that, in the cluster, there is a Deployment with same name which uses
    a different pod selector.</p><p>Delete or rename the Deployment. Let&#39;s look
    at deleting the Deployment. First, get a list of the Deployments:</p><pre class="hljs
    nginx">kubectl get all<br/>...<br/><br/>NAME                                             TYPE           CLUSTER-IP      EXTERNAL-IP      PORT(S)        AGE<br/>service/kubernetes                               ClusterIP      10.83.240.1     &lt;none&gt;           443/TCP        18d<br/><br/>NAME                                             DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE<br/>deployment.apps/harness-example-deployment       1         1         1            1           4d<br/>...</pre><p>And
    then delete the Deployment:</p><pre class="hljs coffeescript">kubectl delete deploy/harness-example-deployment
    svc/kubernetes<br/><br/>deployment.extensions &#34;harness-example-deployment&#34;
    deleted<br/><br/>service &#34;kubernetes&#34; deleted</pre><p>Rerun the Harness
    deployment and the error should not occur.</p><h4>Cannot Create Property</h4><p>The
    following error message can appear if a property, such as the security settings
    (securityContext) in the pod or container, are located in the wrong place in the
    specification:</p><pre>ConstructorException: Cannot create property=spec for JavaBean=class
    V1StatefulSet</pre><p></p><p>Ensure that your YAML specification is formed correctly.</p><p>For
    steps on how to add a security context for a pod or container, see <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/"
    target="_blank">Configure a Security Context for a Pod or Container</a> from Kubernetes.</p><p>Here
    is an example:</p><pre>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name:
    security-context-demo-2<br/>spec:<br/>  securityContext:<br/>    runAsUser: 1000<br/>  containers:<br/>  -
    name: sec-ctx-demo-2<br/>    image: gcr.io/google-samples/node-hello:1.0<br/>    securityContext:<br/>      runAsUser:
    2000<br/>      allowPrivilegeEscalation: false</pre><h3>Terraform</h3><p>The following
    are resolutions to common configuration problems when <a href="/article/boug6e884h-terraform-provisioning-with-harness">Terraform
    Provisioning with Harness</a>.</p><h4>Provisioned Resources Already Exist (Terraform
    State File Locked)</h4><p>When a <a href="/article/hdclyshiho-run-a-terraform-plan-with-the-terraform-apply-step">Terraform
    Apply</a> step fails because of a timeout, subsequent deployments might see following
    error message:</p><pre>Error creating [object]. The [object] already exists.</pre><p></p><p>Use
    a longer timeout for the Terraform Apply step.</p><p>When the Terraform Apply
    times out, Terraform locks the Terraform state file. A Terraform <a href="https://www.terraform.io/docs/language/state/locking.html#force-unlock"
    target="_blank">Force Unlock</a> needs to be performed.</p><div class="note-callout">Locking
    and unlocking of tfstate files is handled by Terraform automatically. You can
    disable state locking for most commands with the <code>-lock</code> flag but it
    is not recommended. See <a href="https://www.terraform.io/docs/language/state/locking.html"
    target="_blank">State Locking</a> from Terraform.</div><p>After timeout, no resources
    may be added to the state file. A manual cleanup of any resources created must
    be performed as well.</p><h4>TerraformValidation - Terraform validation result:
    false</h4><p>Harness performs the following validation when you use Terraform
    in a deployment:</p><ol><li>Is Terraform installed on the Harness Delegate? Harness
    installs it automatically, but it might have been removed.</li><li>Can the Harness
    Delegate connect to the Git repo?</li></ol><p>If the Harness Delegate does not
    have Terraform installed, you will see a log entry such as the following:</p><pre>2020-04-21
    19:26:19,134 INFO software.wings.delegatetasks.validation.TerraformValidation
    - Running terraform validation for task<br/><br/>2020-04-21 19:26:19,157 INFO
    software.wings.delegatetasks.validation.TerraformValidation - Terraform validation
    result: false</pre><p>The message <code>Terraform validation result: false</code>
    means Terraform is not installed on the Delegate.</p><p>Install Terraform on the
    Delegate to fix this.</p><h3>Harness Secret Managers</h3><p>If the Harness Delegate(s)
    cannot authenticate with a Secret Manager, you might see an error message such
    as this:</p><pre class="hljs bash">Was not able to login Vault using the AppRole
    auth method. <br/>Please check your credentials and try again</pre><p></p><p>For
    most authentication issues, try to connect to the <a href="/article/hngrlb7rd6-harness-secret-manager-overview">Harness
    Secrets Manager</a> from the host running your Harness Delegate(s). This is done
    simply by using a cURL command and the same login credentials you provided when
    you set up the Harness Secret Manager.</p><p>For example, here is a cURL command
    for HashiCorp Vault:</p><pre>curl -X POST -d &#39;{&#34;role_id&#34;:&#34;&lt;APPROLE_ID&gt;&#34;,
    &#34;secret_id&#34;:&#34;&lt;SECRET_ID&gt;&#34;}&#39; https://&lt;HOST&gt;:&lt;PORT&gt;/v1/auth/approle/login</pre><p></p><p>If
    the Delegate fails to connect, it is likely because of the credentials or a networking
    issue.</p><p></p><h3>SAML SSO</h3><p>The following errors might occur during the
    set up or use of SAML SSO.</p><h4>Signed in user is not assigned to a role for
    the Project (Harness)</h4><p>A user registered in the Harness project in Azure
    portal is not able to access the application and gets this error.</p><h5>Cause</h5><p>If
    the email address used in Harness is different from the email address in Azure
    app, you will get an error saying that the user is not assigned to a role for
    the Harness application.</p><h5>Solution</h5><p>Make sure the email address used
    in Harness matches with the email address in Azure app.</p><p>For more information
    about SAML SSO configuration with Azure, see <a href="/article/mlpksc7s6c-single-sign-on-saml">Single
    Sign-On (SSO) with SAML</a>.</p><p></p><h3>Shell Scripts</h3><p>This section covers
    common problems experienced when using the <a href="/article/k5lu0u6i1i-using-shell-scripts">Shell
    Script</a> step.</p><h4>FileNotFoundExeption inside shell script execution task</h4><p>This
    error happens when you are publishing output and your Shell Script step exits
    early from its script.</p><p>If you exit from the script (<code>exit 0</code>),
    values for the context cannot be read.</p><p>If you publish output variables in
    your Shell Script step, structure your script with <code>if...else</code> blocks
    to ensure it always runs to the end of the script.</p><h3>Harness Policy Engine</h3><p>The
    following errors might occur during the set up or use of <a href="/article/1d3lmhv4jl-harness-governance-overview">Harness
    Policy Engine</a>.</p><h4>Policy Evaluation Failed</h4><p>If a Harness Policy
    Engine Policy Set is enabled and your Pipeline or other resource does not pass
    the set, you will receive a failure, like this:</p><p></p><figure><img src="https://files.helpdocs.io/i5nl071jo5/articles/jzklic4y2j/1654622380011/clean-shot-2022-06-07-at-10-19-25.png"
    style="max-height:50%;max-width:50%" data-hd-height="50%" data-hd-width="50%"/></figure><p>Contact
    your Harness account admin to resolve the issue. If the Policy Set is in error,
    you can disable it by locating the Policy Set and toggling the <strong>Enforced</strong>
    setting off:</p><p></p><figure><img src="https://files.helpdocs.io/i5nl071jo5/articles/jzklic4y2j/1654622856463/clean-shot-2022-06-07-at-10-27-13.png"/></figure><p></p>'
  slug: troubleshooting
  tags: []
  is_live: true
