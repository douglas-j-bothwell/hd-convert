<p>This topic provides settings and permissions for the Kubernetes Cluster Connector.</p><p>The Kubernetes Cluster Connector is a platform-agnostic connection to a Kubernetes cluster located anywhere.</p><p>For cloud platform-specific connections, see platform <a href="/category/1ehb4tcksy">Cloud Connectors</a>.</p><div class="note-callout">Looking for the How-to? See <a href="/article/1gaud2efd4-add-a-kubernetes-cluster-connector">Add a Kubernetes Cluster Connector</a>.</div><h3>Video Summary</h3><p>Here&#39;s a 10min video that walks you through adding a Harness Kubernetes Cluster Connector and Harness Kubernetes Delegate. The Delegate is added to the target cluster and then the Kubernetes Cluster Connector uses the Delegate to connect to the cluster:</p><div class="hd--embed" data-provider="YouTube" data-thumbnail="https://i.ytimg.com/vi/wUC23lmqfnY/hqdefault.jpg"><iframe width=" 200" height="150" src="https://www.youtube.com/embed/wUC23lmqfnY?feature=oembed" frameborder="0" allowfullscreen="allowfullscreen"></iframe></div><p></p><h3>Kubernetes Cluster Connector vs Platform Connectors</h3><p>The Kubernetes Cluster Connector is platform-agnostic. Use it to access a cluster on any platform.</p><p>It cannot also access platform-specific services and resources. For those, use a platform Connector like Google Cloud Platform or Amazon Web Services.</p><p>See <a href="/article/cii3t8ra3v-connect-to-google-cloud-platform-gcp">Add a Google Cloud Platform (GCP) Connector</a>, <a href="/article/98ezfwox9u-add-aws-connector">Add an AWS Connector</a>.</p><p>For example, let&#39;s say you have a GKE Kubernetes cluster hosted in Google Cloud Platform (GCP). You can use the Kubernetes Cluster Connector to connect Harness to the cluster in GCP. The Kubernetes Cluster Connector cannot also access Google Container Registry (GCR).</p><p>In this case, you have two options:</p><ol><li>Use a Google Cloud Platform Connector to access the GKE cluster and all other GCP resources you need.</li><li>Set up a Kubernetes Cluster Connector for the GKE cluster. Next, set up a Google Cloud Platform Connector for all other GCP services and resources.</li></ol><p>When you set up a deployment in Harness, you will specify Connector to use for the artifact and target cluster. If we use option 2 above, you will select a Google Cloud Platform Connector for the GCR container. Next, you will select Kubernetes Cluster Connector for the target cluster.</p><p>Which option you choose will depend on how your teams use Harness.</p><h3>Permissions Required</h3><p>The IAM roles and policies needed by the account used in the Connector depend on what operations you are using with Harness and what operations you want Harness to perform in the cluster.</p><p>You can use different methods for authenticating with the Kubernetes cluster, but all of them use a Kubernetes Role.</p><p>The Role used must have either the <code>cluster-admin</code> permission in the target cluster or admin permissions in the target namespace.</p><p>For a detailed list of roles and policies, see <a href="https://ngdocs.harness.io/article/vz5cq0nfg2#role" target="_blank">Harness Role-Based Access Control Overview</a>.</p><h4>Harness CI Permission Requirements</h4><p>If you are only using the Kubernetes Cluster Connector for Harness Continuous Integration (CI), you can use a reduced set of permissions.</p><p>For Harness CI, the Delegate requires CRUD permissions on Secret and Pod.</p><p>Here is a same Service Account and RoleBinding that lists the minimum permissions:</p><pre>apiVersion: v1<br/>kind: Namespace<br/>metadata:<br/>  name: cie-test<br/>---<br/>apiVersion: v1<br/>kind: ServiceAccount<br/>metadata:<br/>  name: cie-test-sa<br/>  namespace: cie-test<br/>---<br/>kind: Role<br/>apiVersion: rbac.authorization.k8s.io/v1<br/>metadata:<br/>  name: sa-role<br/>  namespace: cie-test<br/>rules:<br/>  - apiGroups: [&#34;&#34;]<br/>    resources: [&#34;pods&#34;, &#34;secrets&#34;]<br/>    verbs: [&#34;get&#34;, &#34;list&#34;, &#34;watch&#34;, &#34;create&#34;, &#34;update&#34;, &#34;delete&#34;]<br/>  - apiGroups: [&#34;&#34;]<br/>    resources: [&#34;events&#34;]<br/>    verbs: [&#34;list&#34;, &#34;watch&#34;]<br/>---<br/>kind: RoleBinding<br/>apiVersion: rbac.authorization.k8s.io/v1<br/>metadata:<br/>  name: sa-role-binding<br/>  namespace: cie-test<br/>subjects:<br/>  - kind: ServiceAccount<br/>    name: cie-test-sa<br/>    namespace: cie-test<br/>roleRef:<br/>  kind: Role<br/>  name: sa-role<br/>  apiGroup: rbac.authorization.k8s.io</pre><h4>Builds (CI)</h4><p>A Kubernetes service account with CRUD permissions on Secret, Service, Pod, and PersistentVolumeClaim (PVC).</p><p>For more information, see <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#user-facing-roles" target="_blank">User-Facing Roles</a> from Kubernetes.</p><h4>Deployments (CD)</h4><p>A Kubernetes service account with permission to create entities in the target namespace is required. The set of permissions should include <code>list</code>, <code>get</code>, <code>create</code>, <code>watch</code> (to fetch the pod events), and <code>delete</code> permissions for each of the entity types Harness uses. In general, cluster admin permission or namespace admin permission is sufficient.</p><p>If you don’t want to use <code>resources: [“*”]</code> for the Role, you can list out the resources you want to grant. Harness needs <code>configMap</code>, <code>secret</code>, <code>event</code>, <code>deployment</code>, and <code>pod</code> at a minimum for deployments, as stated above. Beyond that, it depends on the resources you are deploying via Harness.</p><p>If you don’t want to use <code>verbs: [“*”]</code> for the Role, you can list out all of the verbs (<code>create</code>, <code>delete</code>, <code>get</code>, <code>list</code>, <code>patch</code>, <code>update</code>, <code>watch</code>).</p><p>The YAML provided for the Harness Delegate defaults to <code>cluster-admin</code> because that ensures anything could be applied. Any restriction must take into account the actual manifests to be deployed.</p><h3>Harness CI Cluster Requirements</h3><p>For Harness <strong>Continuous Integration</strong>, the resources required for the Kubernetes cluster depends on the number of builds running in parallel, as well as the resources required for each build.</p><p>Below is a rough estimation of the resources required, based on the number of daily builds:</p><p></p><table><tbody><tr><td><p><strong>PRs/Day</strong></p></td><td><p><strong>Nodes with 4 CPU, 8GB RAM,100GB disk</strong></p></td><td><p><strong>Nodes with 8 CPU, 16GB RAM, 200GB disk</strong></p></td></tr><tr><td><p>100</p></td><td><p>19 - 26</p></td><td><p>11 - 15</p></td></tr><tr><td><p>500</p></td><td><p>87 - 121</p></td><td><p>45 - 62</p></td></tr><tr><td><p>1000</p></td><td><p>172 - 239</p></td><td><p>89 - 123</p></td></tr></tbody></table><p></p><h3>Credential Validation</h3><p>When you click <strong>Submit</strong>, Harness uses the provided credentials to list controllers in the <strong>default</strong> namespace in order to validate the credentials. If validation fails, Harness does not save the Connector and the <strong>Submit</strong> fails.</p><p>If your cluster does not have a <strong>default</strong> namespace, or your credentials do not have permission in the <strong>default</strong> namespace, then you can check <strong>Skip default namespace validation</strong> to skip this check and saving your Connector settings.</p><p>You do not need to come back and uncheck <strong>Skip default namespace validation</strong>.</p><p>Later, when you define a target Infrastructure using this Connector, you will also specify a specific namespace. During deployment, Harness uses this namespace rather than the <strong>default</strong> namespace.</p><p>When Harness saves the Infrastructure it performs validation even if <strong>Skip default namespace validation</strong> was checked.</p><h3>Name</h3><p>The unique name for this Connector.</p><h3>ID</h3><p>See <a href="/article/li0my8tcz3-entity-identifier-reference">Entity Identifier Reference</a>.</p><h3>Description</h3><p>Text string.</p><h3>Tags</h3><p>See <a href="/article/i8t053o0sq-tags-reference">Tags Reference</a>.</p><h3>Cluster Details</h3><h4>Manual or Use a Delegate</h4><div class="note-callout"><strong>Recommended:</strong> Install and run the Harness Kubernetes Delegate in the target Kubernetes cluster, and then use the Kubernetes Cluster Connector to connect to that cluster using the Harness Kubernetes Delegate you installed. This is the easiest method to connect to a Kubernetes cluster.</div><p>You can select to enter the authentication details of the target cluster or use the role associated with a Harness Delegate.</p><p>When you select a Delegate, the Harness Delegate will inherit the Kubernetes service account associated with the Delegate pod.</p><p>The service account associated with the Delegate pod must have the Kubernetes <code>cluster-admin</code> role.</p><p>See <a href="/article/f9bd10b3nj-install-a-kubernetes-delegate">Install a Kubernetes Delegate</a>.</p><h4>Master URL</h4><p>The Kubernetes master node URL. The easiest method to obtain the master URL is using kubectl:</p><p><code>kubectl cluster-info</code></p><h4>Authentication</h4><p>Select an authentication method.</p><div class="note-callout">Basic (Username and Password) authentication is not recommended. Basic authentication has been removed in GKE 1.19 and later.</div><h3>Username and Password</h3><p>Username and password for the Kubernetes cluster. For example, <strong>admin</strong> or <strong>john@example.com</strong>, and a Basic authentication password.</p><p>You can use an inline username or a Harness <a href="https://docs.harness.io/article/ygyvp998mu-use-encrypted-text-secrets">Encrypted Text secret</a>.</p><p>For the password, select or create a new Harness Encrypted Text secret.</p><div class="note-callout">This is not used, typically. Some Connectors have Basic authentication disabled by default. The cluster would need Basic authentication enabled and a specific username and password configured for authentication.</div><div class="tip-callout">For OpenShift or any other platform, this is not the username/password for the platform. It is the username/password for the cluster.</div><h3 id="service_account">Service Account</h3><p>Add the service account token for the service account. The token must be pasted in decoded in the Encrypted Text secret you create/select.</p><p>To get a list of the service accounts, run <code>kubectl get serviceAccounts</code>.</p><p>For example, here&#39;s a manifest that creates a new SA named <code>harness-service-account</code> in the <code>default</code> namespace.</p><pre># harness-service-account.yml<br/>apiVersion: v1<br/>kind: ServiceAccount<br/>metadata:<br/>  name: harness-service-account<br/>  namespace: default</pre><p></p><p>Next, you apply the SA.</p><pre>kubectl apply -f harness-service-account.yml</pre><p></p><p>Next, grant the SA the <code>cluster-admin</code> permission (see <strong>Permissions Required</strong> above).</p><pre># harness-clusterrolebinding.yml<br/>apiVersion: rbac.authorization.k8s.io/v1<br/>kind: ClusterRoleBinding<br/>metadata:<br/>  name: harness-admin<br/>roleRef:<br/>  apiGroup: rbac.authorization.k8s.io<br/>  kind: ClusterRole<br/>  name: cluster-admin<br/>subjects:<br/>- kind: ServiceAccount<br/>  name: harness-service-account<br/>  namespace: default</pre><p></p><p>Next, apply the ClusterRoleBinding.</p><pre>kubectl apply -f harness-clusterrolebinding.yml</pre><p></p><p>Once you have the SA added, you can gets its token using the following commands.</p><pre>SERVICE_ACCOUNT_NAME={SA name}<br/><br/>NAMESPACE={target namespace}<br/><br/>SECRET_NAME=$(kubectl get sa &#34;${SERVICE_ACCOUNT_NAME}&#34; --namespace &#34;${NAMESPACE}&#34; -o=jsonpath=&#39;{.secrets[].name}&#39;)<br/><br/>TOKEN=$(kubectl get secret &#34;${SECRET_NAME}&#34; --namespace &#34;${NAMESPACE}&#34; -o=jsonpath=&#39;{.data.token}&#39; | base64 -d)<br/><br/>echo $TOKEN</pre><p></p><p>The <code>| base64 -d</code> piping decodes the token. You can now enter it into the Connector.</p><p></p><h3>OpenID Connect</h3><p>These settings come from the OIDC provider authorization server you have set up and others come from the provider app you are using to log in with.</p><p>First let&#39;s look at the authorization server-related settings:</p><h4>Master URL</h4><p>The issuer URI for the provider authentication server.</p><p>For example, in Okta, this is the Issuer URL for the <a href="https://developer.okta.com/docs/concepts/auth-servers/" target="_blank">Authorization Server</a>:</p><figure><a href="https://files.helpdocs.io/kw8ldg1itf/articles/whwnovprrb/1583352263225/image.png"><img src="https://files.helpdocs.io/kw8ldg1itf/articles/whwnovprrb/1583352263225/image.png" style="max-height:50%;max-width:50%" data-hd-height="50%" data-hd-width="50%"/></a></figure><p>Providers use different API versions. If you want to identify the version also, you can obtain it from the token endpoint.</p><p>In Okta, in the authentication server <strong>Settings</strong>, click the <strong>Metadata URI</strong>. Locate the <strong>token_endpoint</strong>. Use the <strong>token_endpoint</strong> URL except for the <strong>/token</strong> part. For example, you would use <code>https://dev-00000.okta.com/oauth2/default/v1</code> from the following endpoint:</p><pre class="hljs bash">&#34;token_endpoint&#34;:&#34;https://dev-00000.okta.com/oauth2/default/v1/token&#34;</pre><h4>OIDC Username and Password</h4><p>Login credentials for a user assigned to the provider app.</p><ul><li><strong>OIDC</strong> <strong>Client ID:</strong> Public identifier for the client that is required for all OAuth flows. In Okta, this is located in the <strong>Client Credentials</strong> for the app:</li></ul><figure><a href="https://files.helpdocs.io/kw8ldg1itf/articles/whwnovprrb/1583352850791/image.png"><img src="https://files.helpdocs.io/kw8ldg1itf/articles/whwnovprrb/1583352850791/image.png" style="max-height:50%;max-width:50%" data-hd-height="50%" data-hd-width="50%"/></a></figure><h4>OIDC Secret</h4><p>The client secret for the app. For Okta, you can see this in the above picture.</p><h4>OIDC Scopes</h4><p>OIDC scopes are used by an application during authentication to authorize access to a user&#39;s details, like name and picture. In Okta, you can find them in the Authorization Server <strong>Scopes</strong> tab:</p><figure><a href="https://files.helpdocs.io/kw8ldg1itf/articles/whwnovprrb/1583353336258/image.png"><img src="https://files.helpdocs.io/kw8ldg1itf/articles/whwnovprrb/1583353336258/image.png" style="max-height:50%;max-width:50%" data-hd-height="50%" data-hd-width="50%"/></a></figure><p>If you enter multiple scopes, separate them using spaces.</p><p>The remaining OIDC Token settings are part of the provider app you are using to log in.</p><h3>Client Key Certificate</h3><h4>Client Key</h4><p>Create or select a Harness secret to add the client key for the client certificate. The key can be pasted into the secret either Base64 encoded or decoded.</p><h4>Client Key passphrase</h4><p>Create or select a Harness secret to add the client key passphrase. The passphrase can be pasted in either Base64 encoded or decoded.</p><h4>Client Certificate</h4><p>Create or select a Harness secret to add the client certificate for the cluster.</p><p>The public client certificate is generated along with the private client key used to authenticate. The certificate can be pasted in either Base64 encoded or decoded.</p><h4>Client Key Algorithm (optional)</h4><p>Specify the encryption algorithm used when the certificate was created. Typically, RSA.</p><h4>CA Certificate (optional)</h4><p>Create or select a Harness secret to add the Certificate authority root certificate used to validate client certificates presented to the API server. For more information, see <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/" target="_blank">Authenticating</a> from Kubernetes.</p><p></p><h3>Amazon AWS EKS Support</h3><p>AWS EKS is supported using the Inherit Delegate Credentials option in the Kubernetes Cluster Connector settings. You can use your <a href="#service_account">EKS service account</a> token as well.</p><p>To install a delegate in your AWS infrastructure, do the following:</p><ul><li>Install a Harness Kubernetes Delegate in your EKS cluster.<div class="note-callout">You must be logged in as an admin user when you run the <code>kubectl apply -f harness-delegate.yaml</code> command.</div></li><li>Give it a name that you can recognize as an EKS cluster Delegate. For information on installing a Kubernetes Delegate, see <a href="/article/f9bd10b3nj-install-a-kubernetes-delegate">Install a Kubernetes Delegate</a>.</li><li>In the Kubernetes Cluster Connector settings, select the Delegate.</li><li>When setting up the EKS cluster as the target Infrastructure, select the Kubernetes Cluster Connector.</li></ul><h3>OpenShift Support</h3><div class="note-callout">This section describes how to support OpenShift using a Delegate running externally to the Kubernetes cluster. Harness does support running Delegates internally for OpenShift 3.11 or greater, but the cluster must be configured to allow images to run as root inside the container in order to write to the filesystem.</div><p>Typically, OpenShift is supported through an external Delegate installation (shell script installation of the Delegate outside of the Kubernetes cluster) and a service account token, entered in the <strong>Service Account</strong> setting.</p><p>You only need to use the <strong>Master URL</strong> and <strong>Service Account Token</strong> setting in the <strong>Kubernetes Cluster Connector</strong> settings.</p><p>The following shell script is a quick method for obtaining the service account token. Run this script wherever you run kubectl to access the cluster.</p><p>Set the <code>SERVICE_ACCOUNT_NAME</code> and <code>NAMESPACE</code> values to the values in your infrastructure.</p><pre class="hljs bash">SERVICE_ACCOUNT_NAME=default<br/>NAMESPACE=mynamepace<br/>SECRET_NAME=$(kubectl get sa &#34;${SERVICE_ACCOUNT_NAME}&#34; --namespace &#34;${NAMESPACE}&#34; -o json | jq -r &#39;.secrets[].name&#39;)<br/>TOKEN=$(kubectl get secret &#34;${SECRET_NAME}&#34; --namespace &#34;${NAMESPACE}&#34; -o json | jq -r &#39;.data[&#34;token&#34;]&#39; | base64 -D)<br/>echo $TOKEN</pre><p>Once configured, OpenShift is used by Harness as a typical Kubernetes cluster.</p><h5>OpenShift Notes</h5><ul><li>If you decide to use a username/password for credentials in the Harness Kubernetes Cluster Connector, do not use the username/password for the OpenShift platform. Use the username/password for the <strong>cluster</strong>.</li><li>Harness supports <a href="https://docs.openshift.com/container-platform/4.1/applications/deployments/what-deployments-are.html" target="_blank">DeploymentConfig</a>, <a href="https://docs.openshift.com/enterprise/3.0/architecture/core_concepts/routes.html" target="_blank">Route</a>, and <a href="https://docs.openshift.com/enterprise/3.2/architecture/core_concepts/builds_and_image_streams.html#image-streams" target="_blank">ImageStream</a> across Canary, Blue Green, and Rolling deployment strategies. Please use <code>apiVersion: apps.openshift.io/v1</code> and not <code>apiVersion: v1</code>.</li><li>The token does not need to have global read permissions. The token can be scoped to the namespace.</li><li>The Kubernetes containers must be OpenShift-compatible containers. If you are already using OpenShift, then this is already configured. But be aware that OpenShift cannot simply deploy any Kubernetes container. You can get OpenShift images from the following public repos: <a href="https://hub.docker.com/u/openshift" target="_blank">https://hub.docker.com/u/openshift</a> and <a href="https://access.redhat.com/containers" target="_blank">https://access.redhat.com/containers</a>.</li><li>Useful articles for setting up a local OpenShift cluster for testing: <a href="https://computingforgeeks.com/setup-openshift-origin-local-cluster-on-centos/">How To Setup Local OpenShift Origin (OKD) Cluster on CentOS 7</a>, <a href="https://chrisphillips-cminion.github.io/kubernetes/2019/07/08/OpenShift-Redirect.html" target="_blank">OpenShift Console redirects to 127.0.0.1</a>.</li></ul><h3>YAML Example</h3><pre>connector:<br/>  name: Doc Kubernetes Cluster<br/>  identifier: Doc_Kubernetes_Cluster<br/>  description: &#34;&#34;<br/>  orgIdentifier: &#34;&#34;<br/>  projectIdentifier: &#34;&#34;<br/>  tags: {}<br/>  type: K8sCluster<br/>  spec:<br/>    credential:<br/>      type: ManualConfig<br/>      spec:<br/>        masterUrl: https://00.00.00.000<br/>        auth:<br/>          type: UsernamePassword<br/>          spec:<br/>            username: john.doe@example.io<br/>            passwordRef: account.gcpexample</pre><p></p>